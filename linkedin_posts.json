[
  {
    "author": "Paulo Rosado",
    "description": "Meus amigos e minhas amigas dev iniciante… deixa eu te perguntar uma coisa bem séria:\nVocê tá programando, tá aprendendo, tá estudando — mas quando abre um projeto cheio de pasta, você sabe onde tá pisando?\nSabe o que é esse tal de src, controllers, services, models?\nOu você só entra, roda o projeto e torce pra não quebrar nada?\nPois então vamo conversar. Vamo abrir essa caixa preta. Vamo entender de verdade.\nPorque estrutura de pastas não é frescura.\nNão é enfeite.\nÉ clareza. É organização. É profissionalismo.\nQuando você entende a estrutura de um projeto, você anda mais rápido, colabora melhor, quebra menos a cabeça e ainda mostra que tá pronto pra jogar no time de verdade.\n\"src\" é onde o coração bate.\n\"models\" é onde mora a estrutura dos seus dados.\n\"services\" é onde a regra do jogo acontece.\n\"controllers\" é quem comanda a orquestra.\nCada pasta tem seu papel. Cada parte tem sua função.\nE quando todo mundo respeita a estrutura, o projeto flui, a equipe cresce, e o caos vai embora.\nQuer escrever código limpo? Comece organizando as pastas.\nQuer crescer na carreira? Entenda o projeto que você pisa.\nQuer ser respeitado como dev? Mostre que você entende mais que sintaxe.\nOrganização, meu amigo, é revolução no código.\nEntão me diz:\nVocê tá pronto pra dominar seu projeto? Ou vai continuar perdido em meio a pasta e arquivo sem saber o que faz o quê?",
    "image_path": "linkedin_images/post_image_0.jpg"
  },
  {
    "author": "Priyanka Vergadia",
    "description": "🛑 𝐒𝐓𝐎𝐏 𝐛𝐮𝐢𝐥𝐝𝐢𝐧𝐠 𝐀𝐈 𝐚𝐠𝐞𝐧𝐭𝐬 𝐟𝐫𝐨𝐦 𝐬𝐜𝐫𝐚𝐭𝐜𝐡. Instead use this repository: 40+ production-ready agent implementations with complete source code, from basic conversational bots to enterprise multi-agent systems.\n𝐖𝐡𝐚𝐭 𝐜𝐚𝐮𝐠𝐡𝐭 𝐦𝐲 𝐚𝐭𝐭𝐞𝐧𝐭𝐢𝐨𝐧:\n↳\nLangGraph AI\nworkflows with state management examples\n↳ Self-healing code agents that debug themselves\n↳ Multi-agent research teams using AutoGen\n↳ Memory-enhanced systems with episodic + semantic storage\n↳ Advanced RAG with controllable retrieval strategies\n𝐓𝐡𝐞 𝐭𝐞𝐜𝐡𝐧𝐢𝐜𝐚𝐥 𝐝𝐞𝐩𝐭𝐡 𝐢𝐬 𝐢𝐦𝐩𝐫𝐞𝐬𝐬𝐢𝐯𝐞:\n↳ Vector embeddings with\nPinecone\n/\nChroma\nDB integration\n↳ Async processing patterns for concurrent agent execution\n↳\nPydantic\nmodels for structured agent outputs\n↳ Real-world error handling and retry mechanisms\nEach implementation includes:\n✅ Complete notebooks with explanations\n✅ Architecture diagrams and workflow logic\n✅ Integration patterns for popular frameworks\n✅ Performance optimization techniques\nThis is essentially a master class in agent engineering disguised as a GitHub repo by\nNir Diamant\n. Perfect for AI engineers who want to understand how these systems work and where to get started.\n🔗 Repository:\nhttps://lnkd.in/dmGE-t_6\nWhich agent architecture are you most curious about? The multi-agent collaboration patterns are fascinating.\n♻️ If you found this useful: I regularly share Cloud & AI insights(through my newsletter subscribe\nhttps://lnkd.in/dRifnnex\n) hit follow (\nPriyanka Vergadia\n) and feel free to share it so others can learn too!\nhashtag\n#\nAIEngineering\nhashtag\n#\nLangChain\nhashtag\n#\nLangGraph\nhashtag\n#\nMultiAgent\nhashtag\n#\nMachineLearning\nhashtag\n#\nRAG\nhashtag\n#\nVectorDB\nhashtag\n#\nOpenAI\nhashtag\n#\nAi\nhashtag\n#\nAIEngineer\nhashtag\n#\nAIAgents\nhashtag\n#\nagenticai",
    "image_path": "linkedin_images/post_image_1.jpg"
  },
  {
    "author": "Pedro Lima",
    "description": "O visual importa pro usuário! ⚡\nOlhe para os cartões e compare, qual você consegue ter mais informações?\nClaro que essa resposta é simples, MAS são o mesmos que estão visualmente mais agradáveis, né? Essa é a ideia do post, não basta termos informações em tela, a forma que elas são expostas contam muito pro usuário, e esse detalhe não pode ser descartado.\nNo Power BI é possível fazer de diferentes formas, como as comparadas, então é legal usarmos o melhor sempre que possível para nossas entregas não serem só úteis, mas agradáveis visualmente também. 😁\nAproveite o post que mostro os detalhes principais para você chegar ao resultado, nesse caso, usamos até DAX.\nBons estudos, Padawans! 🚀",
    "image_path": "linkedin_images/post_image_2.jpg"
  },
  {
    "author": "Kauan Oliveira",
    "description": "Eu sei que você 🫵 já pesquisou algo completamente errado e mesmo assim encontrou o que procurava em aplicativos como Spotify, Netflix ou Amazon.\nNão, não tem ninguém lendo seus pensamentos, na real tem um sistema muito interessante e bem desenvolvido por trás, o Elasticsearch!\nEscrevi um documento explicando como ele funciona utilizando o Spotify como exemplo, e a designer\nMelissa Cavalcante\nfez um lindo design pra melhorar a absorção da informação, recomendo dar uma olhada no perfil dela!\nCaso o assunto te interesse, recomendo pesquisar um pouco sobre Embedding Semântico, uma técnica usada até no ChatGPT que distribui palavras em vetores para entender contextos e palavras escritas erradas, mas só se você não quiser ficar com um pouco de medo da inteligência artificial 🤭\nMuito obrigado pela leitura e bons estudos!",
    "image_path": "linkedin_images/post_image_3.jpg"
  },
  {
    "author": "Lucíola Coelho",
    "description": "A\nhashtag\n#\nMicrosoft\nacaba de lançar uma série GRATUITA de 18 episódios sobre IA generativa.\nIdeal para pessoas que são novas na IA e querem começar a aprender.\nAqui estão 5 episódios que se destacaram\nVocê levará menos de 1,5 horas para assistir a todos estes:\n👉 Introdução à IA generativa e LLMs\nhttps://lnkd.in/dxds5CXY\n👉 Explorando e comparando diferentes LLMs\nhttps://lnkd.in/dnu5sP68\n👉 Entendendo os fundamentos da engenharia de prompt\nhttps://lnkd.in/d8t56acG\n👉 Criando aplicativos de IA low-code\nhttps://lnkd.in/dKVXmdeK\n👉 Agentes de IA – Apresenta Agentes de IA, onde os LLMs podem realizar ações por meio de ferramentas ou estruturas.\nhttps://lnkd.in/d8VKw7Ve\nReposte esta postagem para ajudar outras pessoas em sua rede.\n◾ É Hoje!!! Abrimos as inscrições às 10h.\nDia 1º de Julho a\nhashtag\n#\nEscolaIA\nvai abrir as inscrições em um formato novo!\nSiga\nLucíola Coelho\npara Aprender com uma\nhashtag\n#\nEngenheiraIA\n.\nhashtag\n#\nEscolaIA\nhashtag\n#\nA1ªEscolaIAdoBrasil\nhashtag\n#\nAgênciaAIIA\nhashtag\n#\nA1ªAgênciaIAdoBrasil\nhashtag\n#\nCursoIA\nhashtag\n#\nHotmart\nhashtag\n#\nCertificaçãoIA",
    "image_path": "linkedin_images/post_image_4.jpg"
  },
  {
    "author": "Ronnan Lima",
    "description": "🚨 NOTÍCIA URGENTE PARA DADOS E IA🚨\nDatabricks\nlançou algo que PARECE MENTIRA — mas é 100% real:\nDatabricks\nFree Edition acabou de chegar.\nE ela vai explodir as portas da inteligência de dados para o mundo todo.\n(gratuito. ilimitado. sem pegadinhas.)\nVeja o que você pode fazer agora — sem pagar um centavo:\n1 - Construir dashboards, agentes de IA e modelos preditivos\n2 - Aprender habilidades práticas em engenharia de dados e IA\n3 - Usar a mesma plataforma que empresas gigantes utilizam\n4 - Colaborar com uma comunidade global de profissionais\nE o melhor: Totalmente gratuito. Para sempre.\n(Apenas uso pessoal, sem fins comerciais)\nSe você é estudante, curioso ou futuro pro da área de dados...\nEssa é sua chance de aprender como os profissionais...com as ferramentas dos profissionais.\nVeja o anúncio oficial do CEO da Databricks,\nAli Ghodsi\n:\n👉\nhttps://lnkd.in/dEiP9dhF\nEstaaaaaa esperando o queeeee?\nE você pode começar agora:\nhttps://lnkd.in/dADxaSZ7",
    "image_path": "linkedin_images/post_image_5.jpg"
  },
  {
    "author": "Sérgio Henrique",
    "description": "🚨 Você criou um dashboard incrível, mas… e a documentação?\nQuem nunca abriu um dashboard e ficou se perguntando:\nQual era o objetivo disso aqui mesmo?\nQue dado é esse?\nComo navega?\nQuem fez?\nA verdade é que dashboards incríveis também precisam ser documentados. Sem isso, viram labirintos de gráficos bonitos… e confusos.\nPensando nisso, aqui vai um guia prático (e visual!) com os principais elementos que você precisa documentar:\nObjetivo do Dashboard\nExplique claramente por que ele existe. O que ele responde? Qual decisão ele orienta?\nPúblico-alvo\nPara quem foi feito? Ajuda a garantir que a comunicação esteja adequada.\nFontes de dados\nListe as origens dos dados utilizados. Transparência gera confiança.\nDefinições dos indicadores\nDescreva cada métrica apresentada. Nada de “taxa X” sem contexto!\nEstrutura do dashboard\nComo as informações estão organizadas? Isso facilita o entendimento geral.\nInstruções de acesso\nGuia simples para quem está entrando pela primeira vez.\nFiltros e interações\nExplique como o usuário pode navegar. Tem drill-down? Tem filtros? Mostre como usar!\nResponsáveis\nQuem criou, mantém, ou pode responder dúvidas sobre o dashboard?\nHistórico de atualizações\nManter um log das mudanças ajuda a entender variações de dados com o tempo.\nLimitações conhecidas\nNenhum dashboard é perfeito. Ser honesto sobre as limitações é sinal de maturidade analítica.\n💡 Dica final: Documentar bem um dashboard é um ato de empatia com o próximo analista… e com você mesmo no futuro!\nhashtag\n#\nDataViz\nhashtag\n#\nBI\nhashtag\n#\nDashboard\nhashtag\n#\nAnáliseDeDados\nhashtag\n#\nDataLiteracy\nhashtag\n#\nComunicaçãoDeDados\nhashtag\n#\nLinkedIn",
    "image_path": "linkedin_images/post_image_6.jpg"
  },
  {
    "author": "Erickson Lopes",
    "description": "🚀 Dica de Python: Criando agentes inteligentes com CrewAI! 🤖🐍\nVocê já ouviu falar do CrewAI? É uma biblioteca poderosa para criar agentes de IA colaborativos, ideal para tarefas autônomas, análises, resumos, e até automação de decisões.\nNesta dica, montei um exemplo simples que responde à clássica pergunta:\n\"Por que o céu é azul?\" ☁️🔵\nCom poucos blocos de código, criamos:\n🧠 Um agente com papel, objetivo e histórico definidos\n✅ Uma tarefa clara com descrição e saída esperada\n🤝 Uma equipe para executar tudo de forma orquestrada\nEssa estrutura é perfeita para explorar a criação de assistentes, copilotos e automações inteligentes com modelos da OpenAI via LangChain.\n➡️ Repositório:\nhttps://lnkd.in/dYE2WPDJ\n↩️ Dica anterior:\nhttps://lnkd.in/dU6HbTDD\n🔗 Se curtiu, salva aí pra testar depois!\nhashtag\n#\nPython\nhashtag\n#\nInteligênciaArtificial\nhashtag\n#\nCrewAI\nhashtag\n#\nLangChain\nhashtag\n#\nDesenvolvimentoDeSoftware\nhashtag\n#\nAutomação\nhashtag\n#\nDicasDePython\nhashtag\n#\nIA",
    "image_path": "linkedin_images/post_image_7.jpg"
  },
  {
    "author": "Leo Candido",
    "description": "A Apple resolveu jogar um balde de água fria na capacidade de raciocínio dos LLMs. O Famoso \"pensando...\".\nQue eles realmente não pensam a gente já sabia. Mas esse estudo traz coisas bem interessantes que valem a leitura do artigo e com certeza a discussão também.\nEnquanto OpenAI, Anthropic e Google promovem seus “modelos que pensam” como avanços rumo à AGI, o artigo da Apple traz uma outra perspectiva pra narrativa.\nO artigo “The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity” traz essa perspectiva.\nEles testaram modelos em puzzles clássicos com complexidade crescente (Tower of Hanoi, River Crossing, Blocks World etc).\nA ideia? Avaliar não só se o modelo acerta, mas como ele \"pensa\" durante o processo.\nResultado?\n- Em tarefas simples, os modelos sem raciocínio foram melhores.\n- Em tarefas médias, os LRMs (Large Reasoning Models) com “chain-of-thought” se destacaram.\n- Em tarefas complexas, todos colapsaram. Inclusive os mais sofisticados.\nQuanto mais difícil o problema, menos tokens os modelos usaram para pensar.\nOu seja, quando mais precisam raciocinar... eles desistem.\nA crítica da Apple basicamente é reforçar que o que muitos interpretam como raciocínio, pode ser na prática, apenas um alinhamento estatístico cada vez mais convincente, um comportamento que simula lógica, mas sem compreender de fato o que está fazendo.\nIsso obviamente não significa que os modelos são inúteis. Nem que o estudo encerra a discussão.\nOpenAI, Anthropic e Google têm feito avanços reais e práticos na integração de IA em ambientes corporativos e aplicações do mundo real.\nMas talvez a palavra “raciocínio” esteja sendo usada com generosidade demais.\nO estudo é uma contribuição interessante não por desacreditar os modelos, mas por desafiar a narrativa de que eles já entendem o que fazem.\nComenta aí. Bora debater.\nLink do estudo nos comentários!",
    "image_path": "linkedin_images/post_image_8.jpg"
  },
  {
    "author": "Bruno Aranda",
    "description": "Você começa a montar seu modelo de dados…\nE trava.\n(acontece com todo mundo)\nA dúvida é sempre a mesma:\n👉 Isso aqui é fato ou é dimensão?\nSe você já passou por isso, esse carrossel é pra você.\nSem teoria demais.\nSem blá-blá-blá.\nSó o essencial → com exemplos visuais.\nMostro na prática como as tabelas se conectam — e por quê.\nSe você quer dominar modelagem de dados,\nesse conteúdo vai te poupar horas de tentativa e erro.\n📌 Salva pra rever depois\n👇 E comenta: qual parte da modelagem mais te confunde?\nhashtag\n#\nPowerBI\nhashtag\n#\nSQL\nhashtag\n#\nModelagemDeDados\nhashtag\n#\nAnalytics\nhashtag\n#\nBusinessIntelligence",
    "image_path": "linkedin_images/post_image_9.jpg"
  },
  {
    "author": "Sabrina Otoni",
    "description": "Hoje a nossa comunidade teve a honra de receber dois excelentes profissionais da área de Data Analytics:\nCaio Costa\n(analista de dados do\nItaú Unibanco\n) e\nCaio Guimarães\n(analista de dados da\nRappi\n). Deram um show trazendo uma visão muito rica para nós sobre os desafios e o dia a dia da área nesses diferentes setores.\nForam muitos insights, mas quero destacar especialmente uma reflexão sobre a importância da comunicação clara e assertiva na hora de apresentar resultados e análises para diferentes stakeholders. Saber contextualizar suas escolhas, traduzindo dados em decisões estratégicas compreensíveis para qualquer nível organizacional, é um dos principais diferenciais na nossa carreira em dados.\nA troca ao final da palestra foi incrível... a proximidade, a troca genuína e os relatos pessoais mostraram que, de fato, não existe crescimento sozinho. Receber feedbacks tão positivos só reforça o quanto estamos no caminho certo, construindo uma comunidade colaborativa onde todos evoluem juntos.\nSe você também quer crescer conosco e participar de momentos como esse, fica aqui meu convite especial: venha fazer parte da roubAI Community, é só me mandar uma mensagem! nós ficaremos muito felizes em te ter conosco. 😊",
    "image_path": "linkedin_images/post_image_10.jpg"
  },
  {
    "author": "Yan Arcanjo",
    "description": "Arquitetura Medallion na prática: pipelines de dados com clareza, escalabilidade e propósito\nNa engenharia de dados, aplicar uma arquitetura em camadas como a Medallion — Staging (Landing Zone) > Bronze > Silver > Gold > Platinum — é essencial para transformar dados brutos em ativos confiáveis, rastreáveis e prontos para gerar valor ao negócio.\nAqui está como aplico esse modelo no processamento de tabelas fato:\n1. Staging (Landing Zone) – ingestão bruta, sem transformações\nCom Apache Airflow, orquestro a ingestão diária dos últimos X dias da origem (banco, API, arquivos). Essa abordagem incremental permite capturar alterações retroativas sem reprocessar tudo. Os dados chegam em formato bruto (geralmente Parquet), preservando sua integridade original.\n2. Bronze – estrutura padronizada e confiável\nUtilizando PySpark, processamos os dados da Staging com upsert por partições de data, removendo duplicidades e garantindo consistência. Essa camada mantém os dados historizados e tecnicamente organizados.\n3. Silver – dados tratados e integrados\nAqui realizamos transformações intermediárias: joins, enriquecimentos, padronizações, filtros e ajustes de schema. Os dados ganham uma estrutura analítica básica, prontos para exploração e validações.\nPara esse tratamento, além do PySpark, também é possível utilizar o dbt, que facilita a criação de transformações SQL versionadas, reutilizáveis e documentadas.\n4. Gold – a camada do negócio\nNa Gold é onde todas as regras de negócio são aplicadas.\nAs tabelas são construídas para atender diretamente às necessidades das áreas, com cálculos, hierarquias, métricas, dimensões e agregações definidas.\nO dbt também pode ser utilizado aqui para garantir organização, governança e testes automatizados nos modelos de negócio.\n5. Platinum – produtos de dados especializados\nVisões refinadas para data science, modelos preditivos, análises avançadas ou relatórios estratégicos sob demanda. É a camada que entrega inteligência de forma estruturada.\nOs dados podem ser armazenados tanto em ambientes em nuvem como o BigQuery quanto em data warehouses locais, como SQL Server ou PostgreSQL, dependendo da arquitetura e infraestrutura da empresa.\nA visualização final é feita no Power BI, conectando-se diretamente às camadas Gold e Platinum, garantindo dashboards performáticos, confiáveis e atualizados com base em pipelines auditáveis.\nEssa arquitetura — orquestrada com Airflow, transformada com PySpark e dbt, armazenada em ambientes locais ou cloud, e visualizada via Power BI — entrega soluções escaláveis e sustentáveis para dados em produção.\nhashtag\n#\nEngenhariaDeDados\nhashtag\n#\nArquiteturaMedallion\nhashtag\n#\nStaging\nhashtag\n#\nLandingZone\nhashtag\n#\nAirflow\nhashtag\n#\nPySpark\nhashtag\n#\ndbt\nhashtag\n#\nBigQuery\nhashtag\n#\nSQLServer\nhashtag\n#\nPostgreSQL\nhashtag\n#\nPowerBI\nhashtag\n#\nDataPipeline\nhashtag\n#\nDataOps\nhashtag\n#\nVagasTI",
    "image_path": "linkedin_images/post_image_11.jpg"
  },
  {
    "author": "Vitoria Freire",
    "description": "Como usei Machine Learning para apoiar decisões de crédito com mais precisão e segurança\nEmpresas que operam com concessão de crédito enfrentam um desafio claro: antecipar o risco de inadimplência antes da aprovação.\nNeste projeto, desenvolvi uma solução preditiva baseada em dados reais, com resultados consistentes e replicáveis.\nObjetivo do projeto\nConstruir um modelo de Machine Learning capaz de classificar clientes com maior ou menor risco de inadimplência com base em atributos como renda, comportamento de pagamento e perfil financeiro.\nO que foi feito:\n- Análise exploratória com destaque para renda, dívidas e atrasos.\n- Transformação de variáveis e normalização de renda (log1p).\n- Treinamento e otimização com validação cruzada estratificada (GridSearchCV).\nComparação de modelos:\n- Regressão Logística\n- Random Forest\n- XGBoost\n- LightGBM\nResultados (AUC):\n- XGBoost: 0.86\n- LightGBM: 0.86\n- Random Forest: 0.86\n- Logistic Regression: 0.79\nTodos os modelos de árvore obtiveram excelente desempenho, com precisão, recall e F1-score equilibrados.\nO Random Forest, por exemplo, alcançou F1 de 0.78 com apenas 10 níveis de profundidade.\nPrincipais insights:\n- debt_ratio > 0.6 → forte indicador de inadimplência (>50%).\n- Atrasos anteriores são os melhores preditores de risco.\n- Faixas etárias de 30–50 anos concentram maior volume de inadimplência.\n- Aplicação de regras de crédito por faixa de risco aumenta segurança na concessão.\nTecnologias utilizadas:\n- Python, Pandas, NumPy, Scikit-Learn, XGBoost, LightGBM, Matplotlib, Seaborn\n- Validação com GridSearchCV + StratifiedKFold\nAplicações diretas\n- Classificação de risco automatizada no momento da solicitação\n- Apoio à tomada de decisão em CRMs, sistemas de onboarding ou renegociação\n- Base para construção de APIs de scoring e painéis executivos com Streamlit\n📎 Projeto completo no Kaggle:\nhttps://lnkd.in/d5SHVctE\nhashtag\n#\nDataScience\nhashtag\n#\nMachineLearning\nhashtag\n#\nCreditScoring\nhashtag\n#\nXGBoost\nhashtag\n#\nLightGBM\nhashtag\n#\nFintech\nhashtag\n#\nPython\nhashtag\n#\nModelagemPreditiva\nhashtag\n#\nProjetosDeDados\nhashtag\n#\nCiênciaDeDados",
    "image_path": "linkedin_images/post_image_12.jpg"
  },
  {
    "author": "Quant Insider",
    "description": "Why it's better to use Seasonal ARIMAX (SARIMAX)  than ARIMA for seasonal forecasting\nThe Seasonal ARIMAX (SARIMAX) model extends ARIMA by incorporating seasonal components to forecast time series that display both trend and seasonality.\nWhile ARIMA uses p (autoregressive order), d (differencing for stationarity), and q (moving average order) to capture trends, it cannot model seasonal cycles. SARIMAX adds seasonal parameters—P (seasonal autoregressive order), D (seasonal differencing order), Q (seasonal moving average order), and m (the seasonal period, such as 12 for monthly data with yearly seasonality)—resulting in the model notation SARIMAX(p, d, q)(P, D, Q)m.\nThe model assumes that the series becomes stationary after differencing and that residuals are white noise, ensuring all significant patterns are captured.\nSARIMAX is generally better than ARIMA for seasonal forecasting because it\nIncorporation of Seasonal Components:\nSARIMAX directly integrates seasonal components into the model using seasonal parameters (P, D, Q, s).\nARIMA handles trends but does not include seasonal components, requiring manual differencing to address seasonality.\nHandling of External Variables:\nSARIMAX supports exogenous variables, allowing the model to include external factors such as economic indicators or weather conditions.\nThe inclusion of these variables can further improve forecast accuracy.\nModel Flexibility and Applicability:\nSARIMAX is more flexible and powerful for datasets with clear seasonal behavior (e.g., monthly or quarterly cycles).\nIt naturally models recurring seasonal patterns, making it more appropriate for seasonal forecasting.\nTypical Domains of Application:\nSARIMAX is widely used in sales forecasting, financial market analysis, and inventory management, where seasonal patterns are significant.\nHyperparameter Sensitivity and Tuning:\nModel performance in SARIMAX is highly sensitive to hyperparameter selection.\nTuning is often performed using tools like AIC/BIC and grid search to find the optimal model parameters.\nTo learn how to implement a Time series model in quantitative finance, enroll in the Quantitative Finance Accelerator Program is built in Collaboration with LAQSA (\nLambda Quant\n).\nPankaj Mani(Jha)\nRishi Kohli\nQuantitative Finance Accelerator Program is a comprehensive, cohort-based course designed to take you from the fundamentals to advanced quantitative finance expertise. This program covers a broad range of topics, starting with the essential principles of statistics, mathematics, and coding and advancing through quantitative analysis, modeling, risk management, data science, AI/ML applications, portfolio management, and algorithmic trading strategies.\nEnrollment is open now:\nhttps://lnkd.in/gk-jp-vk\nCourse Brochure Link -\nhttps://lnkd.in/gyhRqfBH\nEarly bird discount of 20%. Use coupon code \"EARLYQFA20\"\nFor queries\nPhone number - 8349489231\nEmail -\neducation@quantinsider.io",
    "image_path": "linkedin_images/post_image_13.jpg"
  },
  {
    "author": "Quant Insider",
    "description": "List of types of Regression of Generalised Linear Models (GLMS), including explanations and use case examples.\nLogistic Regression\nUsed for binary classification tasks where the response variable takes values 0 or 1.\nIt models the log-odds of the probability using the logistic (sigmoid) function.\nCommonly applied in credit scoring, fraud detection, and churn prediction.\nMultinomial Logistic Regression\nGeneralization of logistic regression to handle multi-class problems with more than two possible outcomes.\nIt models the probabilities of each class using a softmax function.\nUseful in text classification, image recognition, and recommendation systems.\nPoisson Regression\nDesigned for modeling count data, where the dependent variable represents the number of occurrences of an event.\nAssumes the response variable follows a Poisson distribution with a log link function.\nUsed in insurance claim prediction, call center traffic, and public health incident rates.\nGamma Regression\nSuitable for modeling continuous, positive, right-skewed data where the variance increases with the mean.\nAssumes a Gamma distribution for the response and often uses an inverse or log link.\nCommon in modeling waiting times, survival data, and insurance payouts.\nProbit Regression\nSimilar to logistic regression, but models the inverse of the standard normal cumulative distribution function.\nPreferred when the latent variable is assumed to follow a normal distribution.\nApplied in econometrics, choice modelling, and psychological testing.\nIf you want to build a career in Quant Finance, enroll now in our Quantitative Finance Accelerator (QFA) built in Collaboration with LAQSA (\nLambda Quant\n)\nQuantitative Finance Accelerator Program is a comprehensive, cohort-based course designed to take you from the fundamentals to advanced quantitative finance expertise. This program covers a broad range of topics, starting with the essential principles of statistics, mathematics, and coding and advancing through quantitative analysis, modeling, risk management, data science, AI/ML applications, portfolio management, and algorithmic trading strategies.\nThe course is taught by industry practitioners with over 20+ years of experience, ensuring practical insights and real-world applications\nWhat You’ll Learn:\n✔ Core concepts in statistics, probability, and time series\n✔ Python for financial applications\n✔ Quant models & derivatives pricing\n✔ Machine learning & AI in finance\n✔ Portfolio theory, risk management, and algo trading\n✔ Final Capstone Project to apply what you've learned\nAccessible via our custom Web + Mobile App with lifetime access to:\nRecorded lectures\nAnnotated notes\nPython labs\nOffline learning\nCourse Link -\nhttps://lnkd.in/gTwQkrRt\nCourse Brochure Link -\nhttps://lnkd.in/gyhRqfBH\nWe are giving an early bird discount of 25%. Use coupon code \"QFA25\"\nFor queries\nPhone - 8349489231\nEmail -\neducation@quantinsider.io",
    "image_path": "linkedin_images/post_image_14.jpg"
  },
  {
    "author": "Felipe Coutinho",
    "description": "Você não precisa ser um gênio da matemática para começar em Machine Learning. Sério mesmo.\nQuando comecei minha jornada em ML, achava que precisava dominar cálculo, álgebra linear e estatística antes de escrever uma linha de código. Resultado? Procrastinação e ansiedade.\nA verdade é que você pode começar HOJE mesmo com conhecimentos básicos.\nPara quem está construindo modelos e aplicações (e não criando novos algoritmos), bibliotecas como NumPy, scikit-learn e TensorFlow fazem o trabalho pesado matemático por você.\nÉ como dirigir um carro, você não precisa entender o funcionamento do motor para chegar ao seu destino.\nMas atenção: em algum momento, a matemática se torna crucial. Quando seu modelo não converge ou você precisa interpretar coeficientes corretamente, aquele conhecimento matemático faz toda diferença.\nConstrua projetos e aprenda matemática PARALELAMENTE:\n✦ Álgebra Linear: matrizes, vetores, operações básicas\n✦ Cálculo: derivadas, gradientes e a regra da cadeia\n✦ Estatística: distribuições, variância e teorema de Bayes\nRecursos que facilitam o aprendizado:\n→ Three Blue One Brown (YouTube): visualizações incríveis\n→ StatQuest: explicações claras e diretas\n→ Cursos da Coursera: Mathematics for Machine Learning\nUma dica é tentar traduzir  fórmulas para linguagem simples que fazem mais sentido para você. Y = aX + b vira \"resultado = inclinação × entrada + ponto inicial\".\nComece construindo. Treine modelos. Explore dados. A matemática virá naturalmente quando você precisar resolver problemas reais.\nE se você quer acelerar sua jornada em dados com conteúdo prático e acessível, a Universidade dos Dados oferece um programa completo por apenas 12x de R$31,64 no plano anual. Uma comunidade com mais de 1400 estudantes e 99% de avaliações positivas te espera!\nCompartilhe se esse post te ajudou a diminuir seu medo da matemática em ML! 🚀",
    "image_path": "linkedin_images/post_image_15.jpg"
  },
  {
    "author": "Vânia Paula",
    "description": "Assista as aulas abaixo (organizadas na sequência que você deve estudar, do básico ao avançado):\nA1 – Iniciante:\nlnkd.in/exfQ_XDq\nA2 – Básico:\nlnkd.in/eb4fnF9f\nB1 – Intermediário:\nlnkd.in/e8XVJg-Q\nB2 – Intermediário Superior:\nlnkd.in/edTjGqHA\nC1 – Avançado:\nlnkd.in/eQf88gJr\nC2 – Proficiente:\nlnkd.in/e9EFRaNu\n↳ Planejamento de 3 meses:\nlnkd.in/eVdaTYhk\n↳ Planejamento de 6 meses:\nlnkd.in/eNivwP26\n↳ Planejamento de 12 meses:\nlnkd.in/ezQC7QZ3\n📖 Quer apostilas gratuitas? Temos também:\nlnkd.in/eZ4cVSua\nCompartilhe e ajude sua rede 💙👥",
    "image_path": "linkedin_images/post_image_16.jpg"
  },
  {
    "author": "Reed Iury",
    "description": "🚀 Card com Efeito Glass no Power BI\nFala, pessoal! 👋\nSe você curte visuais diferentes no Power BI, preparei um template especial com efeito glassmorphism – aquele efeito com transparência e desfoque. Tá prontinho pra usar!\n✨ O que tem no template?\n✅ Efeito vidro (blur + transparência).\n✅ Design responsivo que se adapta a qualquer layout.\n✅ Ícones e animações pra deixar tudo mais fluido.\n✅ Cores personalizáveis (ótimo pra destacar métricas boas/ruins).\n💡 Como usar?\n1️⃣ Baixe o .PBIX (link nos comentários)\n2️⃣ Ajuste os valores direto nas medidas DAX\n3️⃣ Personalize as cores e deixe com a cara do seu projeto\nAh, e se conhece alguém que curte dashboards criativos, marca aqui! 🔁\nhashtag\n#\nPowerBI\nhashtag\n#\nDashboard\nhashtag\n#\nDataViz\nhashtag\n#\nBusinessIntelligence\nhashtag\n#\nTecnologia\nhashtag\n#\nDesign\nhashtag\n#\nHTML",
    "image_path": "linkedin_images/post_image_17.jpg"
  },
  {
    "author": "Renata Costa",
    "description": "Acabou as desculpas de “não sei por onde começar a estudar estatística”",
    "image_path": "linkedin_images/post_image_18.jpg"
  },
  {
    "author": "Avi Chawla",
    "description": "11 plots in data science that are used 90% of the time\n(with precise usage👇)\nVisualizations are critical in understanding complex data patterns and relationships.\nThey offer a concise way to understand the:\n- intricacies of statistical models\n- validate model assumptions\n- evaluate model performance, and much more.\nThe visual below depicts the 11 most important and must-know plots in data science:\n1) KS Plot:\n- It is used to assess the distributional differences.\n- The core idea is to measure the maximum distance between the cumulative distribution functions (CDF) of two distributions.\n- The lower the maximum distance, the more likely they belong to the same distribution.\n- Thus, instead of a “plot”, it is mainly interpreted as a “statistical test” to determine distributional differences.\n2) SHAP Plot:\n- It summarizes feature importance to a model’s predictions by considering interactions/dependencies between them.\n- It is useful in determining how different values (low or high) of a feature affect the overall output.\n3) ROC Curve:\n- It depicts the tradeoff between the true positive rate (good performance) and the false positive rate (bad performance) across different classification thresholds.\n4) Precision-Recall Curve:\n- It depicts the tradeoff between Precision and Recall across different classification thresholds.\n5) QQ Plot:\n- It assesses the distributional similarity between observed data and theoretical distribution.\n- It plots the quantiles of the two distributions against each other.\n- Deviations from the straight line indicate a departure from the assumed distribution.\n6) Cumulative Explained Variance Plot:\n- It is useful in determining the number of dimensions we can reduce our data to while preserving max variance during PCA.\n7) Elbow Curve:\n- The plot helps identify the optimal number of clusters for the k-means algorithm.\n- The point of the elbow depicts the ideal number of clusters.\n8) Silhouette Curve:\n- The Elbow curve is often ineffective when you have plenty of clusters.\n- Silhouette Curve is a better alternative, as depicted above.\n9) Gini-Impurity and Entropy:\n- They are used to measure the impurity or disorder of a node or split in a decision tree.\n- The plot compares Gini impurity and Entropy across different splits.\n- This provides insights into the tradeoff between these measures.\n10) Bias-Variance Tradeoff:\n- It is used to find the right balance between the bias and the variance of a model against complexity.\n11) PDP:\n- Depicts the dependence between target and features.\n- A plot between the target and one feature forms → 1-way PDP.\n- A plot between the target and two feature forms → 2-way PDP.\n👉 Over to you: Do you find it easier to interpret plots over numbers?\n____\nIf you want to learn AI/ML engineering, get this free PDF (530+ pages) with 150+ core DS/ML lessons.\nGet here:\nhttps://lnkd.in/gi6xKmDc\n____\nFind me →\nAvi Chawla\nEvery day, I share tutorials and insights on DS, ML, LLMs, and RAGs.",
    "image_path": "linkedin_images/post_image_19.jpg"
  },
  {
    "author": "John Paz",
    "description": "🚀 Como os Modelos de IA (LLMs) REALMENTE funcionam? 🤯⬇️\nOs grandes modelos de linguagem (LLMs) como ChatGPT não \"pensam\", mas fazem algo incrivelmente poderoso. Vamos simplificar:\n🔹 Tokenização & Embeddings\n📌 O texto é quebrado em tokens (pequenos pedaços).\n📌 Cada token vira um vetor em um espaço multidimensional, onde palavras similares ficam próximas.\n🔹 Mecanismo de Atenção (Self-Attention)\n🔍 O modelo pesa o contexto, diferenciando \"banco\" (financeiro) de \"banco\" (de praça).\n🔍 Cada palavra influencia as outras dinamicamente.\n🔹 Rede Neural Profunda & Processamento\n🧠 Após a atenção, os tokens passam por várias camadas que refinam o significado.\n🧠 Quanto mais camadas, melhor o entendimento do contexto.\n🔹 Previsão & Geração de Texto\n🔢 O modelo gera a próxima palavra baseado em probabilidades, escolhendo a mais adequada a cada momento.\n💡 Por que isso importa?\n🔹 Compreender esses mecanismos é essencial para criar soluções de IA escaláveis e confiáveis.\n🔹 Toda decisão de um LLM vem dessas camadas e da forma como elas processam padrões.\n🎥 Quer entender melhor? Recomendo assistir e salvar esse vídeo:\n👉\nhttps://lnkd.in/dAviqK_6\nO que você acha mais fascinante nos LLMs? Vamos discutir nos comentários! 💬👇\nO futuro já chegou. Para se manter à frente do jogo, siga\nJohn Paz\n.\nhashtag\n#\nIA\nhashtag\n#\nMachineLearning\nhashtag\n#\nLLMs\nhashtag\n#\nDeepLearning\n#Inovação#\nNuvia\nCreditos: Andreas Horn",
    "image_path": "linkedin_images/post_image_20.jpg"
  },
  {
    "author": "Renata Costa",
    "description": "Da série: coisas que aprendi e que me ajudam muito como Analista de BI\nProva de Conceito (POC)\nA primeira vez que ouvi esse termo foi na documentação da Microsoft, e achei incrível sua praticidade e aplicabilidade — não só em projetos de BI, mas em diversas outras áreas.\nUma POC (Proof of Concept) é uma implementação inicial de um design, com escopo e maturidade limitados. Quando bem executada, permite identificar e lidar com complexidades (ou exceções) que poderiam passar despercebidas no início do projeto.\nNo início do projeto, recomenda-se reunir os stakeholders para discutir os seguintes pontos:\n1️⃣ Objetivos e escopo do projeto\n2️⃣ Origem dos dados\n3️⃣ Demonstração inicial (mockup ou draft)\n4️⃣ Ferramentas de desenvolvimento\n5️⃣ Critérios de sucesso\n6️⃣ Critérios de falha\nDiscutam, registrem e validem! A prova de conceito é um documento vivo, que deve ser revisitado constantemente para garantir que os conceitos definidos no início ainda fazem sentido.\nEla é essencial para evitar retrabalho e alinhar as expectativas de todas as partes envolvidas.\nAgora, reparem no mockup: ele foi construído com base na POC. Ou seja, deve seguir os objetivos e escopo definidos, bem como atender aos critérios de sucesso estabelecidos.\nIMPORTANTE!\n📌 Você não precisa esperar ser um(a) analista de dados para começar a usar a POC! Eu trouxe aqui um exemplo que estou incluindo no meu portfólio: me coloquei no lugar do stakeholder, estudei o processo e elaborei minha POC. Esse tipo de iniciativa agrega muito valor ao seu portfólio e demonstra que, além do domínio das ferramentas, você tem uma visão estratégica dos processos!\nQuero saber de vocês! 👇\nJá conheciam a prova de conceito? Já usaram sem saber? Ou agora vão aproveitar para colocar em prática? (Porque, mais uma vez: estudo sem prática é entretenimento).\nE se você quer aprender mais sobre a área de Dados e BI, me siga aqui! Toda semana compartilho algo novo com a rede. 😉\nhashtag\n#\nprovadeconceito\nhashtag\n#\nanalisededados\nhashtag\n#\nbusinessintelligence\nhashtag\n#\npowerbi\n.",
    "image_path": "linkedin_images/post_image_21.jpg"
  },
  {
    "author": "Akshay Pachaar",
    "description": "Effortless Document Parsing with Docling!\nDocling is an open-source Python package that transforms any document into LLM ready data!\nKey Features:\n🔍 OCR for scanned PDFs\n🗂️ PDF, PPTX, DOCX & more → Markdown, JSON\n📑 Advanced PDF parsing: layout, reading order, tables\n🤖 Integrates with LlamaIndex & LangChain\nComing Soon:\n♾️ Equation & code extraction\n🦜🔗 Native LangChain extension\n📝 Metadata extraction (titles, authors, references & language)\nCompatible with macOS, Linux, and Windows on both x86_64 and arm64 architectures.\nGitHub repo:\nhttps://lnkd.in/gWgDw6Cr\n↓\nInterested in ML/AI Engineering? Sign up for our newsletter for in-depth lessons and get a FREE eBook with 150+ core DS/ML lessons:\nhttps://lnkd.in/gB7yHExC",
    "image_path": "linkedin_images/post_image_22.jpg"
  },
  {
    "author": "Karine Lago",
    "description": "Como usar a função ALL da forma mais eficiente, aplicação da DIVIDE, usando a SELECTEDVALUE, evitando o uso da IFERROR e ISERROR, habituando-se com a ISBLACK e melhores práticas de usos de filtros.\nVocê vai aprender tudo isso nesse post que merece ser salvo pra consultar depois!\n💬 Me conta aqui nos comentários: Quantas técnicas que mencionei acima você aprendeu agora?\nhashtag\n#\nDAX\nhashtag\n#\nPowerBI\nhashtag\n#\nMicrosoftPowerBI\nhashtag\n#\nDashboards\nhashtag\n#\nAnalytics\nhashtag\n#\nBusinessIntelligence",
    "image_path": "linkedin_images/post_image_23.jpg"
  },
  {
    "author": "Ajay Shenoy",
    "description": "Interested in GenAI/LLMs?\n🔥🔥 444 datasets🔥🔥\n774.5 TB ║ 8 Languages ║ 32 domains ║\nPre-training ❋ Fine tuning ❋ Evaluation\nYang Liu and co-authors, a big thank you 🙏\n(Links in the comment section)",
    "image_path": "linkedin_images/post_image_24.jpg"
  },
  {
    "author": "Leonardo Miralles",
    "description": "🛠️ A importância dos fluxogramas\nUm fluxograma é uma ferramenta essencial na visualização de processos, ajudando a organizar e entender o fluxo de atividades ou dados em um sistema ou projeto. Apesar de sua grande utilidade, os fluxogramas são muitas vezes subestimados, especialmente quando se trata de empresas que negligenciam a importância da documentação adequada. Documentar processos de forma clara, usando fluxogramas, não só melhora a eficiência, como também facilita a comunicação entre equipes.\n📊 Aqui estão os principais símbolos e seus significados:\n-Elipse (ou Oval): Representa o início e o fim de um processo.\n-Seta: Indica o fluxo ou direção de um processo.\n-Retângulo: Usado para processos ou instruções que envolvem cálculos e atribuição de valores.\n-Paralelogramo: Simboliza a entrada e saída de dados no processo.\n-Losango: Indica a tomada de decisões e escolhas, onde o fluxo pode seguir por diferentes caminhos.\n📈 O uso de fluxogramas deve ser valorizado nas empresas, pois simplifica processos complexos e melhora a clareza de comunicação!\nhashtag\n#\nfluxograma\nhashtag\n#\ndocumentação\nhashtag\n#\nprocessos\nhashtag\n#\neficiência\nhashtag\n#\ngestãodeprojetos\nhashtag\n#\nmelhorespráticas\nhashtag\n#\norganização\nhashtag\n#\nprodutividade",
    "image_path": "linkedin_images/post_image_25.jpg"
  },
  {
    "author": "Akshay Pachaar",
    "description": "Ever seen one of those moving bubbles charts❓\nHere’s how you can create one in Python with just 3 lines of code using D3Blocks!\nWhat you'll need:\n- A Pandas DataFrame where each row represents a sample state at a specific timestamp.\n- Utilize this DataFrame in the `movingbubbles()` method.\nThis will generate a moving bubbles chart like the one shown in the video below.\n----\nIf you enjoyed this, you should also check this FREE Data Science e-book featuring 150 essential lessons in DS and ML:\nblog.dailydoseofds.com\n----\nFind me →\nhttps://lnkd.in/em_V4unu\n✔️\nFor more insights & tutorials on AI and Machine Learning.",
    "image_path": "linkedin_images/post_image_26.jpg"
  },
  {
    "author": "Leonardo Miralles",
    "description": "📌 Métodos Importantes da biblioteca Pandas no Python\nAqui está uma breve descrição de cada comando em três categorias: Data Importing, Data Cleaning e Data Statistic. Esses métodos são essenciais para quem trabalha com análise de dados usando Python.\n🔹 Data Importing\npd.read_csv() – Lê arquivos em formato CSV.\npd.read_table() – Lê arquivos delimitados por tabulações ou outros delimitadores.\npd.read_excel() – Lê planilhas Excel em diferentes formatos (.xls, .xlsx).\npd.read_sql() – Executa consultas SQL e importa os dados para um DataFrame.\npd.read_json() – Lê arquivos em formato JSON e converte em DataFrame.\npd.read_html() – Extrai tabelas de arquivos HTML.\npd.DataFrame() – Cria um DataFrame a partir de listas, dicionários ou arrays.\npd.concat() – Junta DataFrames em um só, concatenando-os.\npd.series() – Cria uma Series (estrutura unidimensional) no Pandas.\npd.date_range() – Gera uma sequência de datas em um intervalo específico.\n🔹 Data Cleaning\npd.fillna() – Preenche valores ausentes (NaN) com um valor especificado.\npd.dropna() – Remove linhas ou colunas com valores ausentes.\npd.sort_values() – Ordena o DataFrame com base nos valores de uma coluna.\npd.apply() – Aplica uma função a cada elemento do DataFrame.\npd.groupby() – Agrupa os dados em grupos com base em uma ou mais colunas.\npd.append() – Anexa linhas de outro DataFrame ao final do atual.\npd.join() – Junta DataFrames com base em uma chave comum.\npd.rename() – Altera os rótulos das colunas ou linhas no DataFrame.\npd.to_csv() – Exporta o DataFrame para um arquivo CSV.\npd.set_index() – Define uma coluna como índice do DataFrame.\n🔹 Data Statistic\npd.head() – Retorna as primeiras n linhas do DataFrame.\npd.tail() – Retorna as últimas n linhas do DataFrame.\npd.describe() – Gera estatísticas descritivas resumidas para o DataFrame.\npd.info\n() – Exibe informações sobre o DataFrame, como número de entradas e tipos de dados.\npd.mean() – Calcula a média para cada coluna numérica.\npd.median() – Calcula a mediana para cada coluna numérica.\npd.count() – Conta o número de entradas não nulas para cada coluna.\npd.std() – Calcula o desvio padrão para cada coluna numérica.\npd.max() – Retorna o valor máximo para cada coluna numérica.\npd.min() – Retorna o valor mínimo para cada coluna numérica.\n💡 Dica: Conhecer e dominar esses métodos ajuda a tornar o trabalho com dados mais eficiente e ágil, garantindo uma análise de dados precisa.\n-------\n🌟 Me siga para vagas e dicas na área de dados\nhashtag\n#\nDataScience\nhashtag\n#\nPython\nhashtag\n#\nPandas\nhashtag\n#\nDataAnalysis\nhashtag\n#\nAnáliseDeDados\nhashtag\n#\nDataCleaning\nhashtag\n#\nDataImporting\nhashtag\n#\nDataStatistics\nhashtag\n#\nMachineLearning\nhashtag\n#\nPythonForDataScience\nhashtag\n#\nManipulaçãoDeDados\nhashtag\n#\nBigData\nhashtag\n#\nAprendizado",
    "image_path": "linkedin_images/post_image_27.jpg"
  },
  {
    "author": "Thaynara Sousa",
    "description": "As tabelas fato e dimensão são essenciais para quem trabalha com análise de dados no Power BI!\nEnquanto a tabela fato concentra os dados numéricos e transacionais, como vendas e lucros, a tabela dimensão oferece o contexto necessário para análise, com informações descritivas como produtos, clientes ou datas.\nA estruturação correta desses componentes não só otimiza o desempenho das análises, como também facilita a criação de relatórios eficientes, permitindo que os insights sejam gerados de forma rápida e precisa.\nSe você quer elevar o nível dos seus dashboards e análises, dominar o uso dessas tabelas é fundamental!",
    "image_path": "linkedin_images/post_image_28.jpg"
  },
  {
    "author": "Pavan Belagatti",
    "description": "The 6 layers of\nhashtag\n#\nGenAI\ntech stack every AI/ML engineer should know.\nThe GenAI technology stack is a sophisticated, layered architecture designed to harness the power of large language models (LLMs) for real-world applications.\nAt its foundation lies the Infrastructure layer, comprising cloud platforms like AWS, Google Cloud, and Azure, or on-premise servers. This layer provides the essential computational resources and scalability needed to support AI operations.\nBuilt upon this is the Data layer, housing databases, data lakes, vector stores, and knowledge graphs. This layer is crucial for managing the vast amounts of data required for training and operating AI systems, ensuring efficient storage, retrieval, and updating of information.\nThe LLM layer sits at the core of the stack, featuring powerful models such as GPT-4, Claude, and BERT. These models serve as the engines of natural language understanding and generation, capable of processing and producing human-like text across various domains.\nThe API layer acts as a bridge, exposing the capabilities of these LLMs through standardized interfaces like the OpenAI API or Anthropic API. This abstraction allows developers to integrate AI capabilities into their applications without dealing with the complexities of model deployment and management.\nThe Application layer leverages frameworks like Langchain and Semantic Kernel to build sophisticated AI applications. These tools provide high-level abstractions for common AI tasks, enabling rapid development of complex, context-aware applications.\nFinally, the UI layer presents these AI capabilities to end-users through web interfaces, mobile apps, chatbots, and voice assistants, making the power of AI accessible and intuitive for non-technical users.\nThis layered approach ensures modularity, scalability, and flexibility, allowing organizations to adapt and evolve their AI systems as technology advances and business needs change.\nAdditional Considerations:\n- Ethical & Responsible AI: Ensure that your GenAI technology stack aligns with ethical guidelines & addresses potential biases.\n- Privacy & Security: Implement robust measures to protect user data & prevent unauthorized access.\n- Scalability & Performance: Design your stack to handle increasing workloads & ensure optimal performance.\n- Cost Optimization: Consider cost-effective options for hardware, software, & cloud services.\nNote: This is not a standard layered approach, you can pick tools, platforms, frameworks as per your use case.\n------------------------------------------------------------\nHaving a robust data platform like\nSingleStore\nto handle not just the vector but any type of data for your AI applications is highly recommended. SingleStore also supports features like hybrid search, real-time analytics, semantic cache, mili second latency, ultra fast ingestion, etc.\nTry SingleStore for FREE:\nhttps://lnkd.in/gCkYgV4H",
    "image_path": "linkedin_images/post_image_29.jpg"
  },
  {
    "author": "Heitor Sasaki",
    "description": "Excel: Ei, isso é uma data né?\nVocê: 47.13 com certeza não é uma data\nExcel: Tem certeza? Parece muito uma data...\nVocê: Não é uma data...\nExcel: Corrigido 🧐\nVocê: 47/13/1900 ???\nExcel: Sempre a seu dispor\n🤡 Quem nunca?",
    "image_path": "linkedin_images/post_image_30.jpg"
  },
  {
    "author": "Fabio Marçolia",
    "description": "Está aproveitando o potencial de IA para tarefas com Dados?\nNo dia a dia dentre muitos desafios, podemos usar IA como um assistente para:\n-Limpar e preparar datasets.\n-Analisar/otimizar erros em códigos.\n-Criar e otimizar pipelines de dados.\n-Documentar arquiteturas de dados.\n-Desenvolver consultas SQL precisas.\n-Gerar relatórios de BI automatizados.\n-Elaborar análises preditivas detalhadas.\nE muito mais.....\nMas para ser eficiente é importante entender como estruturar seus pedidos, usando boas práticas de engenharia de prompts.\nEsse simples framework pode ajudar muito:\n𝟭-𝗔𝘁𝗿𝗶𝗯𝘂𝗮 𝘂𝗺 𝗽𝗮𝗽𝗲𝗹\n-Defina o papel específico para a tarefa (ex.: \"Atue como Engenheiro de Dados\").\n𝟮-𝗘𝘀𝘁𝗮𝗯𝗲𝗹𝗲𝗰̧𝗮 𝗼 𝗼𝗯𝗷𝗲𝘁𝗶𝘃𝗼\n-Explique claramente o que precisa ser entregue (ex.: \"Desenvolva um pipeline ETL\").\n𝟯-𝗖𝗿𝗶𝗲 𝗿𝗲𝘀𝘁𝗿𝗶𝗰̧𝗼̃𝗲𝘀(𝗖𝗮𝘀𝗼 𝗻𝗲𝗰𝗲𝘀𝘀𝗮́𝗿𝗶𝗼)\n-Coloque o que  considerar ou não (ex.: \"Use padrão SQL do SQL Server\").\n𝟰-𝗗𝗲𝘁𝗲𝗿𝗺𝗶𝗻𝗲 𝗼 𝗳𝗼𝗿𝗺𝗮𝘁𝗼\n-Informe como deseja receber a resposta (ex.: \"Apresente em formato de diagrama\").\nDica: Forneça sempre um contexto claro para que suas necessidades ficam claras.\nPS: Utilize IA nesse caso como um apoio, mas não deixe de revisar e ajustar as saídas para garantir a precisão e não prejudicar seu trabalho.\nO que mais podemos fazer?\nCompartilhe e ajude sua rede!",
    "image_path": "linkedin_images/post_image_31.jpg"
  },
  {
    "author": "Lucas Assirati",
    "description": "Dica de python/pandas 🐼: mask()\nO método mask() do pandas é usado para substituir valores em um DataFrame onde uma condição é verdadeira. É uma ferramenta bastante útil para manipular ou limpar dados de forma condicional.\nNo exemplo 1 temos um DataFrame com três colunas contendo números de 1 até 500. Ao utilizar o método df.mask(df < 30), estamos solicitando que os valores menores que 30 no dataframe df original sejam substituídos por NaN no dataframe df_com_mascara destino.\nJá no exemplo 2, temos uma passagem de parâmetro adicional: df.mask(df < 30, -1). Dessa forma, solicitamos que os valores menores que 30 no dataframe df original sejam substituídos por -1 no dataframe df_com_mascara destino, pois foi pedido via passagem de parâmetro, um valor específico para a troca.\nPode-se citar como aplicações práticas para o método:\n- Limpeza de dados, substituindo valores inválidos ou indesejados por NaN ou outros valores;\n- Pré-processamento, preparando dados para análises ao substituir valores que atendem a certas condições;\n- Análises condicionais, ao modificar dados com base em condições específicas para análises mais aprofundadas;\ndentre tantas outras alternativas, dependendo do contexto e do projeto a ser aplicado.\nhashtag\n#\npython\nhashtag\n#\npandas\nhashtag\n#\nmask\nhashtag\n#\ndataframe\nhashtag\n#\nfiltro\nhashtag\n#\nfilter\nhashtag\n#\nreplace",
    "image_path": "linkedin_images/post_image_32.jpg"
  },
  {
    "author": "Rodrigo Santana",
    "description": "Se eu fosse Analista de Dados e quisesse migrar para Engenharia de Dados em 2025 eu seguiria essa trilha…\nSempre re-escrevo esse post, e sempre mudo algo..\nOs conceitos e fundamentos se mantêm, mas é interessante como as percepções mudam.\n💡 Conceitos e Fundamentos:\n- Arquitetura de dados: Conceitos sobre Data Warehouse, Data Lake, Data Lakehouse e Data Marts.\n- Modelagem de Dados: Conceitos sobre Star Schema, Snowflake Schema, Data Vault. Aqui eu focaria em entender as principais diferenças entre os modelos. Vantagens e Desvantagens\nArquitetura Medalhão: bronze, silver, gold.\n- Processamento distribuído: Estudaria sobre processamento distribuído com Spark. Eu iria focar em Spark e leria um livro sobre, um bom livro já cobre os conceitos e a prática de um framework tão usado. O meu foco estaria nele, sei que existem outros mas na minha opinião se dominar bem spark está ótimo.\nArmazenamento de dados: Estudaria sobre formatos: parquet, json, delta, iceberg. O objetivo é entender o básico mesmo, por exemplo: Porque armazenar dados em parquet é interessante para cloud?\n💡 Trabalhando com API's:\nTodo Engenheiro(a) de dados trabalha ou trabalhará com API 's. Então é importante aprender sobre:\n- Como consumir API's.\n- Métodos de autenticação.\n- Como funciona basicamente, entender sobre rate limit, backfilling, webhooks.\n- Threading\n💡 Orquestração de Dados\nEstudaria o principal produto para orquestragem hoje no mercado que é o Apache Airflow.\nEntenderia como esse software funciona, boas práticas para criação de DAG's, como funciona recursos e principais integrações.\n💡 Soluções Modernas (modern data stack)\nEstudaria algumas ferramentas muito faladas ultimamente para compor stacks de dados.\nNão precisa ficar especialista nelas. As inclua no seu projeto de desenvolvimento de estudos e pronto.\nSão elas:\n- airbyte\n- dbt\n- datahub\n- airflow + dbt\n- duckdb\nVocê não precisa ficar expert em todas, é importante entender onde cada uma se encaixa, quando usar e como usar.\n💡Cloud\nEscolha uma das três mais usadas: AWS, Azure e GCP.\n(Eu iria de AWS 😀 )\nEscolha uma delas e aprenda os produtos de dados que são mais usados.\nPense: como fazer um projeto básico na AWS?\nExemplo: Assumindo uma APi pública, como posso subir um código python para consumir uma API, escrever no Data Lake, modelar as tabelas e inserir em um Data Warehouse usando 100% a AWS?\nBom, esse foi um caminho que eu seguiria para migrar de Analista de Dados para Engenheiro de Dados..\nSei que faltou algumas ferramentas ou habilidades na lista, mas tentei considerar que hoje um Analista de Dados já domina e atua com muita coisa..\nEntão, na minha opinião, já tem um bom background, só precisa focar no que pode completar mesmo.\nFaz sentido para você?\nComenta aqui.\nAproveite e envie para um Analista de Dados que está buscando essa migração.\nhashtag\n#\ndataengineering",
    "image_path": "linkedin_images/post_image_33.jpg"
  },
  {
    "author": "Bruno Azambuja",
    "description": "🚀 Gere análises em seu dataframe via Chat com a biblioteca Vanna!\nVanna transforma questões de linguagem natural em análises detalhadas com uso de SQL.\nVanna é uma estrutura Python RAG (Retrieval-Augmented Generation) de código aberto licenciada pelo MIT para geração de SQL e funcionalidades relacionadas.\nPara mais informações dessa biblioteca acesse:\nhttps://lnkd.in/dKscdm3A\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning\nhashtag\n#\nvanna",
    "image_path": "linkedin_images/post_image_34.jpg"
  },
  {
    "author": "Bruno Azambuja",
    "description": "Lux é uma biblioteca Python projetada para simplificar e acelerar o processo de exploração de dados. Ele faz isso sugerindo visualizações e insights quando você exibe um dataframe em um Jupyter Notebook.\nEssas visualizações ajudam a descobrir tendências e padrões interessantes em seus dados, e são apresentadas em um widget interativo, permitindo que você explore e compreenda facilmente seu conjunto de dados, mesmo ao lidar com grandes quantidades de dados.\nLink para saber mais e experimentar -\nhttps://lnkd.in/dxiN6GtF\nhashtag\n#\ntechnology\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmachinelearning\nhashtag\n#\nprogramming\nhashtag\n#\ndatascience",
    "image_path": "linkedin_images/post_image_35.jpg"
  },
  {
    "author": "Heitor Sasaki",
    "description": "Tu já deve tá cansado de ver esse gráfico.\nÉ sempre a mesma coisa:\nA análise descritiva são os analistas...\nA análise preditiva e prescritiva são os cientistas\nblablubla.\nNão me leve a mal, é bom saber isso.\nMas eu quero ir além e interpretar o gráfico de outra maneira.\nConectar esse conhecimento com gestão de mudanças e cultura data driven.\nNa newsletter de amanhã, vou falar mais sobre isso e resumir o que falamos hoje na call do Data Creators.\nInscreva-se para não perder.\nheitorsasaki.substack.com",
    "image_path": "linkedin_images/post_image_36.jpg"
  },
  {
    "author": "Raiane Rosa",
    "description": "🚀 Melhore suas fórmulas DAX no Power BI com o uso de variáveis\nÀ medida que avançamos no aprendizado da Linguagem DAX, é sempre bom que estejamos atentos na otimização de nossas fórmulas para melhorar a legibilidade e o desempenho da ferramenta. Para isso, podemos utilizar variáveis (var), o que torna nossas fórmulas mais claras e facilita a manutenção.\n💡Exemplo Prático:\nSuponha que você queira saber a porcentagem de meses que atingiram a meta de faturamento mensal de $200 milhões. Com variáveis, sua fórmula ficaria assim:\n% Meses Bateram Meta =\nvar MetaFaturamentoMensal = 200000000\nvar qtdTotalMeses = DISTINCTCOUNT('Calendário'[Início do Mês])\nvar qtdMesesBateramMeta = COUNTROWS(\nFILTER(\nVALUES('Calendário'[Início do Mês]),\n[Faturamento Total] >= MetaFaturamentoMensal\n)\n)\nreturn\nDIVIDE(qtdMesesBateramMeta,qtdTotalMeses)\n🚫 Exemplo sem variáveis:\n% Meses Bateram Meta = DIVIDE(COUNTROWS(FILTER(VALUES('Calendário'[Início do Mês]),[Faturamento Total]>=200000000)),DISTINCTCOUNT('Calendário'[Início do Mês]))\nComo você pode notar, o uso de variáveis facilita a organização e entendimento da fórmula. Além disso, como efeito colateral, ainda é capaz de melhorar o desempenho do modelo ao evitar cálculos repetidos.\nhashtag\n#\nPowerBI\nhashtag\n#\nDataAnalytics\nhashtag\n#\nBusinessIntelligence\nhashtag\n#\nAnálisedeDados\nhashtag\n#\nLinguagemDAX\nhashtag\n#\nVariáveis",
    "image_path": "linkedin_images/post_image_37.jpg"
  },
  {
    "author": "Wilson Franquilino",
    "description": "Bora entender....\nEmbora o DAX e o SQL tenham sintaxes e finalidades diferentes, muitas operações podem ser realizadas de maneira semelhante em ambas as linguagens. Compreender como traduzir essas funções entre DAX e SQL pode ajudar a maximizar o uso de ambas as ferramentas para análises de dados mais eficazes.",
    "image_path": "linkedin_images/post_image_38.jpg"
  },
  {
    "author": "Ronnan Lima",
    "description": "Fala pessoal, tudo bom? boa sexta e fiquem com um belo post sobre o Training Day!\nSC-900: 18/06/2024, 10:00 – 14:00 | 19/06/2024, 10:00 – 13:15\nhttps://lnkd.in/dfmY6uN2\nDP-900: 01/07/2024, 10:00 – 13:00 | 02/07/2024, 10:00 – 12:15\nhttps://lnkd.in/d_279mUd\nAZ-900: 16/07/2024, 10:00 – 12:30 | 17/07/2024, 10:00 – 13:00\nhttps://lnkd.in/dNthKA5d\nSC-900: 23/07/2024, 10:00 – 14:00 | 24/07/2024, 10:00 – 13:15\nhttps://lnkd.in/dFymnZAp\nEspero que gostem e compartilhem com os colegas que estão procurando conteúdo para estudar!\nGosta do meu conteúdo aqui? Também estou em outras redes @ronnanlimadataeng :)\nhashtag\n#\ndataengineering\nhashtag\n#\nazure\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\nsql\nhashtag\n#\ncloud\nhashtag\n#\ndatabricks\nhashtag\n#\ndataanalytics\nhashtag\n#\nengenhariadedados\nhashtag\n#\ncienciadedados\nhashtag\n#\nnuvem\nhashtag\n#\nanalisededados",
    "image_path": "linkedin_images/post_image_39.jpg"
  },
  {
    "author": "Riviane Donha",
    "description": "Regressão de Aumento de Gradiente (GBR)\nA GBR vai além de um simples conjunto de modelos, operando como um algoritmo adaptável que aprende com os dados.\nA chave reside na capacidade de aprender com os erros dos modelos anteriores. Cada novo modelo é treinado para corrigir os erros do anterior, refinando as previsões de forma iterativa.\nA GBR se destaca em diversas áreas desafiadoras, como:\nVisão Computacional: Reconhecimento de objetos, detecção de anomalias e análise de imagens médicas.\nProcessamento de Linguagem Natural: Análise de sentimento, classificação de texto e extração de informações.\nFinanças: Previsão de fraudes, análise de risco e detecção de padrões de mercado.\nSazonalidade em Vendas: Previsão da demanda, otimização de estoques e planejamento de campanhas de marketing.\nRobustez: A GBR é robusta a outliers e dados faltantes, tornando-a ideal para problemas do mundo real.\nConjunto de Mil Modelos: A GBR combina até mil modelos de TD em série, como mil especialistas trabalhando juntos, para alcançar a melhor previsão possível.\nSequência de Refinamento: A cada novo modelo, os erros do anterior são utilizados para refinamento, como um time que se aprimora a cada desafio.",
    "image_path": "linkedin_images/post_image_40.jpg"
  },
  {
    "author": "Luana Carvalho",
    "description": "Amigos gravei esse vídeo com muito carinho e temor!\nEu sempre levantarei a bandeira para usarmos a IA, não é atoa que tenho dedicado TODOS OS DIAS da minha vida a estudar e trabalhar com isso.\nO meu sustento depende disso, os meus sonhos serão realizados através do uso da IA, então é com muito temor e verdade que gravei esse vídeo abaixo.\nLembrando que IA vai muuuuuito além do ChatGPT mas isso é só a pontinha do Iceberg.\nEu enquanto profissional dedicada que sou a essa área me sinto no dever de falar e me encontro em meu lugar de fala se tratando disso.\nPor favor, assistam tudo e deixem as suas opniões sobre, só lembrem de me respeitar em quanto pessoa e profissional quando as suas opniões forem divergentes das minhas.\nE pensem sempre na questão ética e moral quando estiverem imersivos nesse universo.\nAvisos:\n📚  Se você tem interesse em adquirir um E-book sobre dados feito por mim, preencha esse formulário que só receberá respostas até hoje, com as suas respostas vou poder direcionar o meu tempo e conhecimento para um conteúdo que realmente faça sentido para você consumir. Preencha aqui\nhttps://lnkd.in/dScagvnb\n✈ Amanhã estarei viajando para SP para um evento na sede do LinkedIn Brasil ♥ e se você quiser ver tudoooo o que vou viver lá, acompanhe os meus story's em meu perfil pessoal no Instagram @iluanacarvalho\nhashtag\n#\ninteligenciaartificial\nhashtag\n#\nchatgpt\nhashtag\n#\nBoletimTech\nhashtag\n#\nLinkedInNotícias",
    "image_path": "linkedin_images/post_image_41.jpg"
  },
  {
    "author": "Karison Avelar",
    "description": "Super dica: 21 comandos mais usados em SQL e sua equivalência em DAX do Power BI\nAprender DAX pode ser um desafio, mas você pode usar seu conhecimento em SQL:\nSELECT -> EVALUATE\nFROM -> RELATED\nWHERE -> FILTER\nORDER BY -> SORT\nGROUP BY -> SUMMARIZE\nHAVING -> CALCULATE\nDISTINCT -> DISTINCT\nIS NULL -> ISBLANK\nIS NOT NULL -> NOT ISBLANK\nCOUNT -> COUNTROWS\nBETWEEN -> BETWEEN\nAVG ->AVERAGEX\nLIKE -> CONTAINS\nSUM -> SUMX\nMAX -> MAX\nMIN ->MIN\nAND -> &&\nOR -> OR\nNOT -> !\nIN -> IN\nPS: Nem todos os casos é uma equivalência perfeita, mas sim uma lógica.\nCrédito:\nFábio Marçolia",
    "image_path": "linkedin_images/post_image_42.jpg"
  },
  {
    "author": "Ronnan Lima",
    "description": "Azure para Analistas de Dados: Seu roteiro para dominar esta stack!\nExame PL-300 - Analista de Dados do Microsoft Power BI\n✔️Pré-requisitos: você deve apresentar insights acionáveis trabalhando com os dados disponíveis e aplicando a experiência no domínio.\n✔️Duração do Exame: 120 minutos\n✔️Número de questões: 40-60\n✔️Nota mínima: 700\n✔️Custo do Exame: $100\n📚Roteiro de estudos:\nhttps://lnkd.in/d2mDViAB\n📚Simulado Oficial:\nhttps://lnkd.in/d3ExfSbD\nExame DP-900 - Microsoft Azure Data Fundamentals\n✔️Pré-requisitos: Familiaridade com os conceitos de dados relacionais e não relacionais.\n✔️Duração do Exame: 60 minutos\n✔️Número de questões: 40-60\n✔️Nota mínima: 700\n✔️Custo do Exame: $60\n📚Roteiro de estudos:\nhttps://lnkd.in/dWKXzBpp\n📚Simulado:\nhttps://lnkd.in/dnj-9rsC\nExame AZ-900 - Microsoft Azure Fundamentals\n✔️Pré-requisitos: Familiaridade com componentes arquitetônicos do Azure e serviços do Azure, como computação, rede e armazenamento.\n✔️Duração do Exame: 60 minutos\n✔️Número de questões: 40-60\n✔️Nota mínima: 700\n✔️Custo do Exame: $60\n📚Roteiro de estudos:\nhttps://lnkd.in/dsCB_WrX\n📚Simulado:\nhttps://lnkd.in/ddfPavCz\nGostou da dica? Compartilhe!\nTambém estou em outras redes:\nhttps://lnkd.in/dBtT2BVF\n*Sobre os simulados, lembrando que antes só existiam questões em inglês agora elas já estão traduzidas.\nhashtag\n#\nazure\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\nsql\nhashtag\n#\ncloud\nhashtag\n#\nanalytics\nhashtag\n#\ndatabricks\nhashtag\n#\ndataanalytics\nhashtag\n#\ndataarchitect\nhashtag\n#\ndataengineering\nhashtag\n#\nai",
    "image_path": "linkedin_images/post_image_43.jpg"
  },
  {
    "author": "Letícia Smirelli",
    "description": "Ao desenvolver relatórios no Power BI é importante levar em conta as necessidades e experiência dos usuários. No post de hoje você vai aprender diversos recursos que podem contribuir para a acessibilidade dos seus projetos, como: paleta de cores, temas de alto contraste, texto alternativo, marcadores e outros! 🎨\nConhece alguma outra dica ou recurso para relatórios mais acessíveis? Contribua nos comentários! 🧙‍♂️😉\n--\nhashtag\n#\npowerbi\nhashtag\n#\nmicrosoftpowerbi\nhashtag\n#\nbusinessinteligence\nhashtag\n#\ndataanalytics\nhashtag\n#\ndatascience\nhashtag\n#\ndatastorytelling\nhashtag\n#\ndashboards\nhashtag\n#\ndashboardpowerbi\nhashtag\n#\ndashboards\nhashtag\n#\nexcel\nhashtag\n#\npowerplatform\nhashtag\n#\nacessibilidade",
    "image_path": "linkedin_images/post_image_44.jpg"
  },
  {
    "author": "Luciano Santos",
    "description": "Sua empresa deixa o líder fazer gestão?\nOutro dia um gestor me disse que não tinha tempo de fazer 1:1s com o time, por todas as outras tarefas que eram demandadas dele. Pedi para ele me listar as tarefas que eram mais importantes do que cuidar do \"bem\" mais importante da empresa: as pessoas.\nSilêncio.\nLíderes e empresas: não existe NADA mais importante do que a liderança focar nas pessoas. Nada. Tudo começa por ali, o resto vem depois.\nFala genial do\nAlê Prates\n(sigam esse cara!)\nDe novo: sua empresa deixa o líder fazer gestão?\nhashtag\n#\nliderança\nhashtag\n#\ngestão\nhashtag\n#\npessoas\nhashtag\n#\nrh",
    "image_path": "linkedin_images/post_image_45.jpg"
  },
  {
    "author": "Filipe Spadetto",
    "description": "Você gostaria de praticar PySpark e Databricks? Se você não sabe existe a versão gratuita do Databricks chamada Databricks Community. Utilizando o notebook e alguns comandos você consegue diversos datasets para análise e tratamento de dados para praticar. É só utilizar o comando para achar uma lista enorme de datasets:\ndisplay(\ndbutils.fs.ls\n(\"/databricks-datasets\"))\nGostou? Bons estudos 😎",
    "image_path": "linkedin_images/post_image_46.jpg"
  },
  {
    "author": "Talles Victor",
    "description": "SQL fundamental em uma página!\nSQL, ou Structured Query Language, é uma linguagem de programação projetada para gerenciar e manipular dados em sistemas de gerenciamento de banco de dados relacionais (RDBMS). É uma linguagem padronizada e amplamente utilizada em bancos de dados relacionais, como MySQL, PostgreSQL, SQL Server, Oracle, SQLite, entre outros.\nSQL é uma linguagem de alto nível e declarativa, o que significa que os usuários podem especificar o que desejam obter, inserir, atualizar ou excluir dos dados, sem precisar se preocupar com os detalhes de como essas operações são realizadas internamente pelo sistema de banco de dados. Em vez disso, o sistema de banco de dados interpreta as instruções SQL e executa as operações de forma eficiente para retornar os resultados desejados.\nAbaixo, os comandos fundamentais para que você possa avançar seu conhecimento nessa linguagem.\nOBS: O texto foi escrito com a ajuda do ChatGPT\n-The grind never stops",
    "image_path": "linkedin_images/post_image_47.jpg"
  },
  {
    "author": "Fabio Marçolia",
    "description": "Os 7 Principais Tipos de Banco de Dados e Quando usar cada um:\n💡Não conhecer pode levar a decisões erradas nas soluções, sendo difícil uma migração depois.\n𝟭-𝗥𝗲𝗹𝗮𝗰𝗶𝗼𝗻𝗮𝗶𝘀 (𝗥𝗗𝗕𝗠𝗦)\n-MySQL, PostgreSQL, Oracle Database, SQL Server\n-Uso: Aplicações empresariais que necessitam de transações complexas e integridade de dados.\n𝟮-𝗡𝗼𝗦𝗤𝗟\n-MongoDB, Cassandra, Redis, Neo4j\n-Uso: Big Data e aplicações em tempo real que necessitam de escalabilidade e flexibilidade.\n𝟯-𝗖𝗼𝗹𝘂𝗻𝗮𝗿𝗲𝘀\n-Apache Cassandra, BigQuery\n-Uso: Análise de grandes volumes de dados, relatórios de business intelligence (BI) e data warehousing, onde a leitura rápida de colunas específicas de grandes tabelas é necessária.\n𝟰-𝗢𝗟𝗔𝗣\n-DuckDB, Analysis Services\n-Uso: Caching e agregações de dados possibilitando uma maior performance e consultas multidimensionais para BI.\n𝟱-𝗚𝗿𝗮𝗳𝗼𝘀\n-Neo4j, ArangoDB\n-Uso: Análise de redes sociais, sistemas de recomendação e detecção de fraudes.\n𝟲-𝗩𝗲𝘁𝗼𝗿𝗶𝗮𝗶𝘀\n-Milvus, Faiss (desenvolvido pelo Facebook AI Research), Elasticsearch (com plugins ou extensões para busca vetorial)\n-Uso: Aplicações de inteligência artificial e machine learning\n𝟳-𝗗𝗮𝗱𝗼𝘀 𝗱𝗲 𝗦𝗲́𝗿𝗶𝗲 𝗧𝗲𝗺𝗽𝗼𝗿𝗮𝗹\n-InfluxDB, TimescaleDB\n-Uso: Monitoramento de sistemas, IoT e análise de séries temporais.\nFaltou algum? O que acredita ser fundamental em conhecimento de banco de dados?\nCompartilhe para ajudar sua rede!",
    "image_path": "linkedin_images/post_image_48.jpg"
  },
  {
    "author": "Clénio Mutunda",
    "description": "𝟱 𝗧𝗢𝗡𝗦 PARA SUBSTITUIR O 𝗕𝗥𝗔𝗡𝗖𝗢 𝗣𝗨𝗥𝗢 NAS SUAS INTERFACES!⬜✨\nO branco puro pode causar fadiga visual, especialmente em ambientes com muita luminosidade, o que gera desconforto e cansaço para os usuários, prejudicando a experiência e diminuindo o tempo de uso do teu produto digital. Por esse motivo, trouxe 5 alternativas que podem ser mais confortáveis para o usuário e um pouco mais elegantes que o clássico branco puro.\nAcompanhe o carrossel!👇🏿\nhashtag\n#\nfff\nhashtag\n#\nuxdesign\nhashtag\n#\nacessibilidade\nhashtag\n#\ndesignconsciente\nhashtag\n#\nclenda",
    "image_path": "linkedin_images/post_image_49.jpg"
  },
  {
    "author": "Tajamul Khan",
    "description": "🎯 𝗦𝗤𝗟 𝗭𝗲𝗿𝗼 𝘁𝗼 𝗛𝗲𝗿𝗼 Notes 𝘄𝗶𝘁𝗵 𝗠𝗶𝗻𝗱𝗺𝗮𝗽 📚\nSQL, or Structured Query Language, is a language used to manage and manipulate relational databases.\nIt may not sound exciting, but its power and flexibility make it an indispensable tool for anyone working with data.\n⚡ Whether you're a business owner trying to make sense of your sales data, a researcher analyzing large datasets, or a software developer building applications that rely on databases, SQL is the way to go.\nWith SQL, you can easily perform complex queries, filter and sort data, and aggregate information to gain insights that would be impossible to obtain manually.\nAnd because SQL is a standardized language, you can use it with virtually any database system, from MySQL and PostgreSQL to Oracle and SQL Server.\nBut don't just take my word for it. Consider the following examples:\n🎇 A small business owner uses SQL to analyze customer data and identify trends that help them make informed decisions about marketing and product development.\n🎇 A researcher uses SQL to join and analyze multiple datasets, uncovering insights that lead to new discoveries and innovations.\n🎇 A software developer uses SQL to build a web application that requires real-time access to a large database, ensuring that users receive accurate and up-to-date information.\nAs you can see, SQL is a versatile and powerful tool that can help you achieve your goals, no matter what they may be. So why not give it a try and see what insights you can uncover? With SQL, the possibilities are endless!\n✨ Follow\nTajamul Khan\nfor more!\nhashtag\n#\nsql\nhashtag\n#\ncommands\nhashtag\n#\ndatabase\nhashtag\n#\nsqldeveloper\nhashtag\n#\nsqlqueries\nhashtag\n#\ndatabasequeries\nhashtag\n#\nsqlprogramming\nhashtag\n#\ndeveloper\nhashtag\n#\nprogrammer\nhashtag\n#\nfunctions\nhashtag\n#\ndataanalytics\nhashtag\n#\ndataanalyst\nhashtag\n#\ndataanalysis\nhashtag\n#\ndata\nhashtag\n#\nprogramming\nhashtag\n#\nlanguage\nhashtag\n#\nsoftwaredeveloper\nhashtag\n#\ndevelopment\nhashtag\n#\nmysql\nhashtag\n#\noracle\nhashtag\n#\nbusiness\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmachinelearning\nhashtag\n#\ntechnology\nhashtag\n#\nai\nhashtag\n#\nml\nhashtag\n#\ndatascience",
    "image_path": "linkedin_images/post_image_50.jpg"
  },
  {
    "author": "Mohibul Alam",
    "description": "🎯 𝗦𝗤𝗟 𝗭𝗲𝗿𝗼 𝘁𝗼 𝗛𝗲𝗿𝗼 Exclusive Hand Notes📚\n👉 SQL, or Structured Query Language, is a language used to manage and manipulate relational databases. It may not sound exciting, but its power and flexibility make it an indispensable tool for anyone working with data.\n⚡ Whether you're a business owner trying to make sense of your sales data, a researcher analyzing large datasets or a software developer building applications that rely on databases, SQL is the way to go.\n👉With SQL, you can easily perform complex queries, filter and sort data and aggregate information to gain insights that would be impossible to obtain manually. And because SQL is a standardized language, you can use it with virtually any database system, from MySQL and PostgreSQL to Oracle and SQL Server.\n⚡ But don't just take my word for it. Consider the following examples:\n🎇 A small business owner uses SQL to analyze customer data and identify trends that help them make informed decisions about marketing and product development.\n🎇 A researcher uses SQL to join and analyze multiple datasets, uncovering insights that lead to new discoveries and innovations.\n🎇 A software developer uses SQL to build a web application that requires real-time access to a large database, ensuring that users receive accurate and up-to-date information.\nAs you can see, SQL is a versatile and powerful tool that can help you achieve your goals, no matter what they may be. So why not give it a try and see what insights you can uncover? With SQL, the possibilities are endless!\n🔥 you can learn SQL from\nMOHNAS\nW3Schools.com\n🔔 Follow\nMohibul Alam\nto learn new things everyday 💡\nhashtag\n#\ndevelopment\nhashtag\n#\nmarketing\nhashtag\n#\nbusiness\nhashtag\n#\ndata\nhashtag\n#\nsales\nhashtag\n#\nsoftwaredeveloper\nhashtag\n#\nhelp\nhashtag\n#\nsmallbusiness\nhashtag\n#\npower\nhashtag\n#\nbuilding\nhashtag\n#\nsql\nhashtag\n#\nmysql\nhashtag\n#\ndatabase\nhashtag\n#\nlanguage\nhashtag\n#\noracle",
    "image_path": "linkedin_images/post_image_51.jpg"
  },
  {
    "author": "Diêgo Oliveira",
    "description": "Acabei de concluir o curso “Fundamentos de Análise de Dados”!",
    "image_path": "linkedin_images/post_image_52.jpg"
  },
  {
    "author": "Diêgo Oliveira",
    "description": "Acabei de concluir  o curso “Fundamentos de Estatística: Parte 1” de\nEddie Davila\n! Confira em:\nhttps://lnkd.in/gDQ4J7Yq\nhashtag\n#\nestatística\n.",
    "image_path": null
  },
  {
    "author": "Gustavo Caetano",
    "description": "Pra galera que está aprendendo a codar ou acabou de iniciar na área de T.I trouxe aqui essas dicas de pontos para ajudar vocês no dia a dia de códigos e também algumas para acrescentar no dia a dia do inglês.\nComentem aqui o que acharam dessas dicas.",
    "image_path": null
  },
  {
    "author": "Diêgo Oliveira",
    "description": "Gostaria de compartilhar que finalizei meu curso de Bacharelado em sistemas de informação na instituição de ensino UniFTC.",
    "image_path": null
  },
  {
    "author": "Diêgo Oliveira",
    "description": "Acabei de concluir  o curso “Quarta Revolução Industrial: Desafios da Computação em Nuvem” de David Carrasco López! Confira em:\nhttps://lnkd.in/djpp_fxU\nhashtag\n#\nindústria40\nhashtag\n#\ncomputaçãoemnuvem\n.",
    "image_path": null
  },
  {
    "author": "Diêgo Oliveira",
    "description": "Gostaria de compartilhar que recebi uma nova certificação: Apache Airflow na Prática: Do ZERO ao DEPLOY, com Python! da empresa\nUdemy Brasil\n!",
    "image_path": null
  },
  {
    "author": "Vânia Paula",
    "description": "Tire todas suas dúvidas de gramática em:\nhttps://lnkd.in/e-jFHmwC",
    "image_path": null
  },
  {
    "author": "Sandeep Jain",
    "description": "Imposter Syndrome affects as many as 58% of tech employees. It prevents people from performing their best by reducing their confidence in their own abilities.\nhashtag\n#\nimpostersyndrome\nhashtag\n#\ntechcareer\nhashtag\n#\ndevelopercommunity\nhashtag\n#\nsoftwareengineer\nhashtag\n#\nmentalhealth",
    "image_path": "linkedin_images/post_image_59.jpg"
  },
  {
    "author": "Rodrigo Braz",
    "description": "Boa noite pessoal,\nVocês sabem o que é OKR ?\nAbaixo elaborei um artigo para explicar um pouco essa metodologia,  que vem sendo utilizadas cada vez mais nas organizaões;\nVale a pena conferir !\nhashtag\n#\nokrs\nhashtag\n#\nagilidade\nhashtag\n#\ncompartilharconhecimento\nhashtag\n#\ndesenvolvimento",
    "image_path": "linkedin_images/post_image_60.jpg"
  },
  {
    "author": "DataV",
    "description": "Colinha de SQL / SQL Cheat Sheet\nComeçou a estudar a SQL?\nQue tal uma colinha para relembrar os comandos.\nVia:\nOdemir Depieri Jr\nComunidade de dados no Telegram:\nhttps://lnkd.in/duveBsTr\nAjuste no material identificado pela comunidade:\n1º coluna dos comandos:\n1) Criar base de dados\n[ CREATE DATABASE MinhaBase; ]\n2) Criando uma tabela\n[ CREATE TABLE NomeTabela ( Coluna1 int, Coluna2 varchar ) ]\n3) Deletar uma tabela\n[ DROP TABLE NomeTabela ]\n4) Atualizar uma tabela\n[ UPDATE NomeTabela; SET Coluna1 = 100; WHERE Coluna2 = 'AlgumValor' ]\nhashtag\n#\ncienciadedados",
    "image_path": "linkedin_images/post_image_61.jpg"
  }
]